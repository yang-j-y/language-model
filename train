# -*- coding: UTF-8 -*-

"""
训练神经网络模型

大家之后可以加上各种的 name_scope（命名空间）
用 TensorBoard 来可视化

==== 一些术语的概念 ====
# Batch size : 批次(样本)数目。一次迭代（Forword 运算（用于得到损失函数）以及 BackPropagation 运算（用于更新神经网络参数））所用的样本数目。Batch size 越大，所需的内存就越大
# Iteration : 迭代。每一次迭代更新一次权重（网络参数），每一次权重更新需要 Batch size 个数据进行 Forward 运算，再进行 BP 运算
# Epoch : 纪元/时代。所有的训练样本完成一次迭代

# 假如 : 训练集有 1000 个样本，Batch_size=10
# 那么 : 训练完整个样本集需要： 100 次 Iteration，1 个 Epoch
# 但一般我们都不止训练一个 Epoch

==== 超参数（Hyper parameter）====
init_scale : 权重参数（Weights）的初始取值跨度，一开始取小一些比较利于训练
learning_rate : 学习率，训练时初始为 1.0
num_layers : LSTM 层的数目（默认是 2）
num_steps : LSTM 展开的步（step）数，相当于每个批次输入单词的数目（默认是 35）
hidden_size : LSTM 层的神经元数目，也是词向量的维度（默认是 650）
max_lr_epoch : 用初始学习率训练的 Epoch 数目（默认是 10）
dropout : 在 Dropout 层的留存率（默认是 0.5）
lr_decay : 在过了 max_lr_epoch 之后每一个 Epoch 的学习率的衰减率，训练时初始为 0.93。让学习率逐渐衰减是提高训练效率的有效方法
batch_size : 批次(样本)数目。一次迭代（Forword 运算（用于得到损失函数）以及 BackPropagation 运算（用于更新神经网络参数））所用的样本数目
（batch_size 默认是 20。取比较小的 batch_size 更有利于 Stochastic Gradient Descent（随机梯度下降），防止被困在局部最小值）
"""
from sklearn.datasets import load_digits
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import  accuracy_score
import tensorflow as tf
import datetime
#from data_read import *
from data_read import Input, load_data, args, save_path, data_path
import numpy as np
#from network import *
from re_mgu import BN_ReMGU

def train(train_data, vocab_size, num_layers, num_epochs, batch_size, model_save_name,
          learning_rate=0.3, max_lr_epoch=10, lr_decay=0.8, print_iter=50):
    # 训练的输入
    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)


    # 创建训练的模型
    m = Model(training_input, is_training=True, hidden_size=300, vocab_size=vocab_size, num_layers=num_layers)

    # 初始化变量的操作
    init_op = tf.global_variables_initializer()

    # 初始的学习率（learning rate）的衰减率
    orig_decay = lr_decay

    with tf.Session() as sess:
        sess.run(init_op)  # 初始化所有变量

        # Coordinator（协调器），用于协调线程的运行
        coord = tf.train.Coordinator()
        # 启动线程
        threads = tf.train.start_queue_runners(coord=coord)

        # 为了用 Saver 来保存模型的变量
        saver = tf.train.Saver(var_list=tf.global_variables()) # max_to_keep 默认是 5, 只保存最近的 5 个模型参数文件
        
        # 开始 Epoch 的训练
        for epoch in range(num_epochs):
            # 只有 Epoch 数大于 max_lr_epoch（设置为 10）后，才会使学习率衰减
            # 也就是说前 10 个 Epoch 的学习率一直是 1, 之后每个 Epoch 学习率都会衰减
            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0)
            m.assign_lr(sess, learning_rate * new_lr_decay)

            # 当前的状态
            # 第二维是 2 是因为对每一个 LSTM 单元有两个来自上一单元的输入：
            # 一个是 前一时刻 LSTM 的输出 h(t-1)
            # 一个是 前一时刻的单元状态 C(t-1)
            current_state = np.zeros(( num_layers,  batch_size, m.hidden_size))

            # 获取当前时间，以便打印日志时用
            curr_time = datetime.datetime.now()
 
            for step in range(training_input.epoch_size):
                # train_op 操作：计算被修剪（clipping）过的梯度，并最小化 cost（误差）
                # state 操作：返回时间维度上展开的最后 LSTM 单元的输出（C(t) 和 h(t)），作为下一个 Batch 的输入状态
                if step % print_iter != 0:
                    cost, _, current_state = sess.run([m.average_cost, m.train_op, m.state], feed_dict={m.init_state: current_state})
                else:
                    seconds = (float((datetime.datetime.now() - curr_time).seconds) / print_iter)
                    curr_time = datetime.datetime.now()
                    cost, _, ppl, current_state = sess.run([m.average_cost, m.train_op, m.ppl, m.state], feed_dict={m.init_state: current_state})
                    # 每 print_iter（默认是 50）打印当下的 Cost（误差/损失）和 Accuracy（精度）
                    print("Epoch {}, 第 {} 步, 损失: {:.3f}, PPL值: {:.3f}, 每步所用秒数: {:.2f}".format(epoch, step, cost, ppl, seconds))
               
            # 保存一个模型的变量的 checkpoint 文件
            saver.save(sess, save_path + '/' + model_save_name, global_step=epoch)
        # 对模型做一次总的保存
        saver.save(sess, save_path + '/' + model_save_name + '-final')

        # 关闭线程
        coord.request_stop()
        coord.join(threads)


class Model(object):
    # 构造函数
    def __init__(self, input_obj, is_training, hidden_size, vocab_size, num_layers,
                 dropout=0.9, init_scale=0.05):
        self.is_training = is_training
        self.input_obj = input_obj
        self.batch_size = input_obj.batch_size
        self.num_steps = input_obj.num_steps
        self.hidden_size = hidden_size


        # 让这里的操作和变量用 CPU 来计算，因为暂时（貌似）还没有 GPU 的实现
        with tf.device("/cpu:0"):
            # 创建 词向量（Word Embedding），Embedding 表示 Dense Vector（密集向量）
            # 词向量本质上是一种单词聚类（Clustering）的方法
            embedding = tf.Variable(tf.random_uniform([vocab_size, 300], -init_scale, init_scale))
            # embedding_lookup 返回词向量 
            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)

        if is_training and dropout < 1:
            inputs = tf.nn.dropout(inputs, dropout)

        inputs = tf.expand_dims(inputs, -1)
            #对输入数据在宽度上进行零填充
        in_bu1 = tf.zeros([self.batch_size,2,hidden_size,1], dtype=tf.float32)
        in_bu2 = tf.zeros([self.batch_size,3,hidden_size,1], dtype=tf.float32)
        in_bu3 = tf.zeros([self.batch_size,4,hidden_size,1], dtype=tf.float32)
        inputs1 = tf.concat([in_bu1,inputs],1)
        inputs2 = tf.concat([in_bu2,inputs],1)
        inputs3 = tf.concat([in_bu3,inputs],1)
           #不同核大小窗口的卷积操作  a1.shape[20,35,1,150]
        a1 = cnn_1(inputs1, 3, hidden_size, 100, is_training)
        a2 = cnn_1(inputs2, 4, hidden_size, 100, is_training)
        a3 = cnn_1(inputs3, 5, hidden_size, 100, is_training)
           #降维
        b1= tf.squeeze(a1)
        b2= tf.squeeze(a2)
        b3= tf.squeeze(a3)
           #卷积结果在向量维度上连接[20,35,256]
        c = tf.concat([b1, b2], 2)
        inputs = tf.concat([c, b3], 2)
        
        # 状态（state）的存储和提取
        # 第二维是 2 是因为对每一个 LSTM 单元有两个来自上一单元的输入：
        # 一个是 前一时刻 LSTM 的输出 h(t-1)
        # 一个是 前一时刻的单元状态 C(t-1)
        # 这个 C 和 h 是用于构建之后的 tf.contrib.rnn.LSTMStateTuple
        self.init_state = tf.placeholder(tf.float32, [num_layers, self.batch_size, self.hidden_size])
        rnn_tuple_state = tuple(self.init_state[idx] for idx in range(num_layers))

        # 创建一个 LSTM 层，其中的神经元数目是 hidden_size 个（默认 650 个）
#        cell = ReMGU(hidden_size,  kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05, seed=2),bias_initializer=tf.random_uniform_initializer(-0.05, 0.05, seed=2))
        cell = BN_ReMGU(hidden_size,is_training=self.is_training,  kernel_initializer=tf.random_uniform_initializer(-0.05, 0.05, seed=2),bias_initializer=tf.random_uniform_initializer(-0.05, 0.05, seed=2))
        # 如果是训练时 并且 Dropout 率小于 1，给 LSTM 层加上 Dropout 操作
        # 这里只给 输出 加了 Dropout 操作，留存率(output_keep_prob)是 0.5
        # 输入则是默认的 1，所以相当于输入没有做 Dropout 操作
        if is_training and dropout < 1:
            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)


        # 如果 LSTM 的层数大于 1, 则总计创建 num_layers 个 LSTM 层
        # 并将所有的 LSTM 层包装进 MultiRNNCell 这样的序列化层级模型中
        # state_is_tuple=True 表示接受 LSTMStateTuple 形式的输入状态
        if num_layers > 1:
            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)
        # dynamic_rnn（动态 RNN）可以让不同迭代传入的 Batch 可以是长度不同的数据
        # 但同一次迭代中一个 Batch 内部的所有数据长度仍然是固定的
        # dynamic_rnn 能更好处理 padding（补零）的情况，节约计算资源
        # 返回两个变量：
        # 第一个是一个 Batch 里在时间维度（默认是 35）上展开的所有 LSTM 单元的输出，形状默认为 [20, 35, 650]，之后会经过扁平层处理
        # 第二个是最终的 state（状态），包含 当前时刻 LSTM 的输出 h(t) 和 当前时刻的单元状态 C(t)
        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)

        # 扁平化处理，改变输出形状为 (batch_size * num_steps, hidden_size)，形状默认为 [700, 650]
        output = tf.reshape(output, [-1, hidden_size]) # -1 表示 自动推导维度大小

        # Softmax 的权重（Weight）
        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))
        # Softmax 的偏置（Bias）
        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))

        # logits 是 Logistic Regression（用于分类）模型（线性方程： y = W * x + b ）计算的结果（分值）
        # 这个 logits（分值）之后会用 Softmax 来转成百分比概率
        # output 是输入（x）， softmax_w 是 权重（W），softmax_b 是偏置（b）
        # 返回 W * x + b 结果
        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)

        # 将 logits 转化为三维的 Tensor，为了 sequence loss 的计算
        # 形状默认为 [20, 35, 10000]
        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])

        # 计算 logits 的序列的交叉熵（Cross-Entropy）的损失（loss）
        loss = tf.contrib.seq2seq.sequence_loss(
            logits,  # 形状默认为 [20, 35, 10000]
            self.input_obj.targets,  # 期望输出，形状默认为 [20, 33]
            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),
            average_across_timesteps=False,
            average_across_batch=True)
        
        weight_decay = 0.001
       
        l2_loss = weight_decay * tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])

        # 更新代价（cost）
        self.cost = tf.reduce_sum(loss) + l2_loss
        self.average_cost = tf.reduce_sum(loss)/self.num_steps

        # Softmax 算出来的概率
        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))

        # 取最大概率的那个值作为预测
        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)

#        计算ppl值
        self.ppl = tf.exp(self.average_cost)


        # 如果是 测试，则直接退出
        if not is_training:
            return

        # 学习率。trainable=False 表示“不可被训练”
        self.learning_rate = tf.Variable(0.01, trainable=False)

        # 返回所有可被训练（trainable=True。如果不设定 trainable=False，默认的 Variable 都是可以被训练的）
        # 也就是除了不可被训练的 学习率 之外的其他变量
        tvars = tf.trainable_variables()
#        print(tvars)

        # tf.clip_by_global_norm（实现 Gradient Clipping（梯度裁剪））是为了防止梯度爆炸
        # tf.gradients 计算 self.cost 对于 tvars 的梯度（求导），返回一个梯度的列表
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)

        # 优化器用 GradientDescentOptimizer（梯度下降优化器）
        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)

        # apply_gradients（应用梯度）将之前用（Gradient Clipping）梯度裁剪过的梯度 应用到可被训练的变量上去，做梯度下降
        # apply_gradients 其实是 minimize 方法里面的第二步，第一步是 计算梯度
        self.train_op = optimizer.apply_gradients(
            zip(grads, tvars),
            global_step=tf.train.get_or_create_global_step())
        
#        with tf.name_scope( 'train_op' ): 
#            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
#            with tf.control_dependencies(update_ops):
#                self.train_op = tf.train.AdamOptimizer( 1e-3 ).minimize( loss ) # 将 损失函数 降到 最低


        # 用于更新 学习率
        self.new_lr = tf.placeholder(tf.float32, shape=[])
        self.lr_update = tf.assign(self.learning_rate, self.new_lr)

    # 更新 学习率
    def assign_lr(self, session, lr_value):
        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})

def cnn_1(input_cnn, filter_sizes, hidden_size, num_filters, is_training):
#    for  filter_size in enumerate(filter_sizes):

    with tf.name_scope("conv"):
        # Convolution Layer
        filter_shape = [filter_sizes, hidden_size, 1, num_filters]
        W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name="W")
        conv1 = tf.nn.conv2d(input_cnn,W,strides=[1, 1, 1, 1],padding="VALID",name="conv1")

        gamma = tf.Variable(tf.ones([num_filters]))
        beta = tf.Variable(tf.zeros([num_filters]))

        pop_mean = tf.Variable(tf.zeros([num_filters]), trainable=False)
        pop_variance = tf.Variable(tf.ones([num_filters]), trainable=False)

        epsilon = 1e-3
        
        def batch_norm_training():
        # 一定要使用正确的维度确保计算的是每个特征图上的平均值和方差而不是整个网络节点上的统计分布值
            batch_mean, batch_variance = tf.nn.moments(conv1, [0, 1, 2], keep_dims=False)

            decay = 0.99
            train_mean = tf.assign(pop_mean, pop_mean*decay + batch_mean*(1 - decay))
            train_variance = tf.assign(pop_variance, pop_variance*decay + batch_variance*(1 - decay))

            with tf.control_dependencies([train_mean, train_variance]):
                return tf.nn.batch_normalization(conv1, batch_mean, batch_variance, beta, gamma, epsilon)

        def batch_norm_inference():
            return tf.nn.batch_normalization(conv1, pop_mean, pop_variance, beta, gamma, epsilon)
        is_training = tf.cast(True, tf.bool)


        batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)
        return tf.nn.relu(batch_normalized_output)
    


if __name__ == "__main__":
    if args.data_path:
        data_path1 = args.data_path
    train_data, valid_data, test_data, vocab_size, id_to_word = load_data(data_path1)

    train(train_data, vocab_size, num_layers=3, num_epochs=50, batch_size=20,
          model_save_name='train-checkpoint')



